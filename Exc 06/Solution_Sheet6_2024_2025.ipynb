{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 1: Load All Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All books have been loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rajubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rajubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"ASoIaF\"\n",
    "\n",
    "# Automatically detect and read all text files in the folder\n",
    "book_files = sorted([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")])\n",
    "\n",
    "# Dictionary to store book contents\n",
    "books = {}\n",
    "\n",
    "# Load books dynamically\n",
    "for file_path in book_files:\n",
    "    book_name = os.path.basename(file_path).replace(\".txt\", \"\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        books[book_name] = file.read()\n",
    "\n",
    "print(\"âœ… All books have been loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ TASK 2: CLEAN TEXT (Remove Metadata, Extra Spaces, and Special Characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove metadata (first 1000 characters usually contain book title and unnecessary info)\n",
    "    text = text[1000:]\n",
    "\n",
    "    # Remove special characters and multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize spaces\n",
    "    text = re.sub(r\"[^\\w\\s']\", \"\", text)  # Keep words and apostrophes only\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Clean all books\n",
    "books_cleaned = {book: clean_text(text) for book, text in books.items()}\n",
    "print(\"âœ… Text cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ TASK 3: SPLIT TEXT INTO CHAPTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chapter splitting completed!\n"
     ]
    }
   ],
   "source": [
    "def split_into_chapters(text):\n",
    "    # Use regex to detect chapter headings (e.g., \"CHAPTER 1\", \"Chapter 2\", etc.)\n",
    "    chapters = re.split(r\"(CHAPTER\\s+\\d+|Chapter\\s+\\d+)\", text)\n",
    "    \n",
    "    # Reconstruct chapters (keep chapter headings)\n",
    "    chapter_list = []\n",
    "    for i in range(1, len(chapters), 2):  # Skip non-matching text\n",
    "        chapter_name = chapters[i]\n",
    "        chapter_text = chapters[i + 1] if i + 1 < len(chapters) else \"\"\n",
    "        chapter_list.append((chapter_name, chapter_text.strip()))\n",
    "\n",
    "    return chapter_list\n",
    "\n",
    "# Apply chapter splitting\n",
    "books_chapters = {book: split_into_chapters(text) for book, text in books_cleaned.items()}\n",
    "print(\"âœ… Chapter splitting completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ TASK 4: WORD FREQUENCY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy English tokenizer model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Increase the max_length limit\n",
    "nlp.max_length = 2000000  # Increase limit to 2 million characters\n",
    "\n",
    "# Function to tokenize text in chunks to avoid max length errors\n",
    "def word_frequency(text, top_n=20, chunk_size=500000):\n",
    "    words = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i : i + chunk_size]  # Process in chunks\n",
    "        doc = nlp(chunk.lower())  # Tokenize and lowercase\n",
    "        words.extend([token.text for token in doc if token.is_alpha])  # Keep only words (no punctuation/numbers)\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "# Compute word frequencies for each book\n",
    "word_frequencies = {book: word_frequency(text) for book, text in books_cleaned.items()}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_word_freq = pd.DataFrame({book: dict(word_freq) for book, word_freq in word_frequencies.items()})\n",
    "df_word_freq.fillna(0, inplace=True)  # Fill NaN with 0\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Word Frequency Analysis\", dataframe=df_word_freq)\n",
    "\n",
    "print(\"âœ… Word frequency analysis completed using SpaCy with chunk processing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ TASK 5: CHARACTER MENTION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define characters to track\n",
    "characters = [\"Jon\", \"Daenerys\", \"Tyrion\", \"Arya\", \"Sansa\", \"Bran\", \"Cersei\", \"Jaime\", \"Stannis\", \"Davos\"]\n",
    "\n",
    "# Function to count character mentions in chunks\n",
    "def count_character_mentions(text, character_list, chunk_size=500000):\n",
    "    mentions = {char: 0 for char in character_list}  # Initialize counts\n",
    "    \n",
    "    for i in range(0, len(text), chunk_size):  # Process in chunks\n",
    "        chunk = text[i : i + chunk_size]  # Get chunk\n",
    "        for char in character_list:\n",
    "            mentions[char] += len(re.findall(r\"\\b\" + char + r\"\\b\", chunk, re.IGNORECASE))  # Count matches\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "# Count character mentions for each book\n",
    "character_mentions = {book: count_character_mentions(text, characters) for book, text in books_cleaned.items()}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_character_mentions = pd.DataFrame(character_mentions)\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Character Mentions\", dataframe=df_character_mentions)\n",
    "\n",
    "print(\"âœ… Character mention analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ TASK 6: COMMON BIGRAMS AND TRIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load SpaCy English tokenizer model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to compute n-grams in chunks\n",
    "def common_ngrams(text, n=2, top_n=15, chunk_size=500000):\n",
    "    ngram_counter = Counter()\n",
    "    \n",
    "    for i in range(0, len(text), chunk_size):  # Process text in chunks\n",
    "        chunk = text[i : i + chunk_size]\n",
    "        doc = nlp(chunk.lower())  # Tokenize with SpaCy\n",
    "        \n",
    "        words = [token.text for token in doc if token.is_alpha and token.text not in stop_words]  # Keep words only\n",
    "        n_grams = list(ngrams(words, n))  # Generate n-grams\n",
    "        ngram_counter.update(n_grams)  # Update counts\n",
    "    \n",
    "    return ngram_counter.most_common(top_n)\n",
    "\n",
    "# Compute bigrams and trigrams for each book\n",
    "bigrams = {book: common_ngrams(text, n=2) for book, text in books_cleaned.items()}\n",
    "trigrams = {book: common_ngrams(text, n=3)}\n",
    "\n",
    "# Convert bigrams to DataFrame\n",
    "df_bigrams = pd.DataFrame({book: {\" \".join(bg[0]): bg[1] for bg in bigrams[book]} for book in bigrams})\n",
    "\n",
    "# Convert trigrams to DataFrame\n",
    "df_trigrams = pd.DataFrame({book: {\" \".join(tg[0]): tg[1] for tg in trigrams[book]} for book in trigrams})\n",
    "\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Common Bigrams\", dataframe=df_bigrams)\n",
    "tools.display_dataframe_to_user(name=\"Common Trigrams\", dataframe=df_trigrams)\n",
    "\n",
    "print(\"âœ… N-gram analysis completed with SpaCy and chunk processing!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
