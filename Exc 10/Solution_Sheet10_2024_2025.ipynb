{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 1: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load the training dataset\n",
    "train_path = \"fake_train_2024_2025.csv\"\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "# Load the testing dataset\n",
    "test_path = \"fake_test_2024_2025.csv\"\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Display both datasets\n",
    "\n",
    "df_train.head()\n",
    "df_test.head()\n",
    "\n",
    "print(\"âœ… Training and Testing datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 2: Preprocessing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load Spacy Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation & special characters\n",
    "    doc = nlp(text)  # Tokenize & Lemmatize\n",
    "    words = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train[\"processed_text\"] = df_train[\"text\"].astype(str).apply(preprocess_text)\n",
    "df_test[\"processed_text\"] = df_test[\"text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Display cleaned data\n",
    "tools.display_dataframe_to_user(name=\"Preprocessed Fake News Training\", dataframe=df_train[[\"text\", \"processed_text\"]])\n",
    "tools.display_dataframe_to_user(name=\"Preprocessed Fake News Testing\", dataframe=df_test[[\"text\", \"processed_text\"]])\n",
    "\n",
    "print(\"âœ… Text preprocessing completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 3: Train a Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare tagged documents\n",
    "train_tagged = [TaggedDocument(words=row.split(), tags=[str(i)]) for i, row in enumerate(df_train[\"processed_text\"])]\n",
    "test_tagged = [TaggedDocument(words=row.split(), tags=[str(i + len(df_train))]) for i, row in enumerate(df_test[\"processed_text\"])]\n",
    "\n",
    "# Combine train and test for model training\n",
    "all_tagged = train_tagged + test_tagged\n",
    "\n",
    "# Train Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(vector_size=300, window=4, min_count=2, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(all_tagged)\n",
    "doc2vec_model.train(all_tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "print(\"âœ… Doc2Vec model trained successfully on the full corpus!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 4: Train Logistic Regression Using Doc2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract embeddings\n",
    "X_train = [doc2vec_model.dv[str(i)] for i in range(len(df_train))]\n",
    "X_test = [doc2vec_model.dv[str(i + len(df_train))] for i in range(len(df_test))]\n",
    "\n",
    "# Get labels\n",
    "y_train = df_train[\"label\"]\n",
    "y_test = df_test[\"label\"]\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Logistic Regression Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 5: Train Doc2Vec Only on Train-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Doc2Vec model only on training data\n",
    "doc2vec_train_only = Doc2Vec(vector_size=300, window=4, min_count=2, workers=4, epochs=20)\n",
    "doc2vec_train_only.build_vocab(train_tagged)\n",
    "doc2vec_train_only.train(train_tagged, total_examples=doc2vec_train_only.corpus_count, epochs=doc2vec_train_only.epochs)\n",
    "\n",
    "# Extract embeddings (test corpus is unobserved)\n",
    "X_train_only = [doc2vec_train_only.dv[str(i)] for i in range(len(df_train))]\n",
    "X_test_only = [doc2vec_train_only.infer_vector(row.split()) for row in df_test[\"processed_text\"]]\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model_train_only = LogisticRegression()\n",
    "lr_model_train_only.fit(X_train_only, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_train_only = lr_model_train_only.predict(X_test_only)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_train_only = accuracy_score(y_test, y_pred_train_only)\n",
    "print(f\"âœ… Logistic Regression Model Accuracy (Train-Only): {accuracy_train_only:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš€ Task 6: Train Word2Vec and Average Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = [row.split() for row in df_train[\"processed_text\"]]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=300, window=4, min_count=2, workers=4)\n",
    "\n",
    "# Function to compute document embeddings by averaging word vectors\n",
    "def document_embedding(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Compute document embeddings\n",
    "X_train_w2v = np.array([document_embedding(row, word2vec_model) for row in df_train[\"processed_text\"]])\n",
    "X_test_w2v = np.array([document_embedding(row, word2vec_model) for row in df_test[\"processed_text\"]])\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model_w2v = LogisticRegression()\n",
    "lr_model_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_w2v = lr_model_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_w2v = accuracy_score(y_test, y_pred_w2v)\n",
    "print(f\"âœ… Logistic Regression Model Accuracy (Word2Vec Averaging): {accuracy_w2v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
